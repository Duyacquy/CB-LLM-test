{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "import os\n", "import gc\n", "import torch\n", "import torch.nn.functional as F\n", "import numpy as np\n", "from transformers import AutoTokenizer, AutoModel\n", "from datasets import load_dataset, concatenate_datasets\n", "import config as CFG\n", "from dataset_utils import preprocess, train_val_test_split\n", "from modules import CBL, RobertaCBL, GPT2CBL\n", "from utils import cos_sim_cubed, get_labels, eos_pooling\n", "import time\n", "from dataset_utils import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser = argparse.ArgumentParser()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "parser.add_argument(\"--dataset\", type=str, default=\"SetFit/sst2\")\n", "parser.add_argument(\"--backbone\", type=str, default=\"roberta\", help=\"roberta or gpt2 or bert\")\n", "parser.add_argument('--tune_cbl_only', action=argparse.BooleanOptionalAction)\n", "parser.add_argument('--automatic_concept_correction', action=argparse.BooleanOptionalAction)\n", "parser.add_argument(\"--labeling\", type=str, default=\"mpnet\", help=\"mpnet, angle, simcse, llm\")\n", "parser.add_argument(\"--cbl_only_batch_size\", type=int, default=64)\n", "parser.add_argument(\"--batch_size\", type=int, default=16)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["parser.add_argument(\"--max_length\", type=int, default=512)\n", "parser.add_argument(\"--num_workers\", type=int, default=4)\n", "parser.add_argument(\"--dropout\", type=float, default=0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ClassificationDataset(torch.utils.data.Dataset):\n", "    def __init__(self, encode_roberta, s):\n", "        self.encode_roberta = encode_roberta\n", "        self.s = s\n", "    def __getitem__(self, idx):\n", "        t = {key: torch.tensor(values[idx]) for key, values in self.encode_roberta.items()}\n", "        y = torch.FloatTensor(self.s[idx])\n", "        return t, y\n", "    def __len__(self):\n", "        return len(self.encode_roberta['input_ids'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_loaders(encode_roberta, s, mode):\n", "    dataset = ClassificationDataset(encode_roberta, s)\n", "    if args.tune_cbl_only:\n", "        batch_size = args.cbl_only_batch_size\n", "    else:\n", "        batch_size = args.batch_size\n", "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=args.num_workers,\n", "                                             shuffle=True if mode == \"train\" else False)\n", "    return dataloader"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n", "    args = parser.parse_args()\n", "    print(args)\n", "    print(\"loading data...\")\n", "    train_dataset, val_dataset, test_dataset = train_val_test_split(args.dataset, CFG.dataset_config[args.dataset][\"label_column\"], ratio=0.2, has_val=False)\n", "    \n", "    print(\"tokenizing...\")\n", "    if args.labeling == 'llm':\n", "        d_list = []\n", "        for i in range(CFG.class_num[args.dataset]):\n", "            d_list.append(\n", "                train_dataset.filter(lambda e: e['label'] == i).select(range(1000 // CFG.class_num[args.dataset])))\n", "        train_dataset = concatenate_datasets(d_list)\n", "        if args.dataset == 'SetFit/sst2':\n", "            d_list = []\n", "            for i in range(CFG.class_num[args.dataset]):\n", "                d_list.append(\n", "                    val_dataset.filter(lambda e: e['label'] == i).select(range(80 // CFG.class_num[args.dataset])))\n", "            val_dataset = concatenate_datasets(d_list)\n", "        print(\"training labeled data len: \", len(train_dataset))\n", "        if args.dataset == 'SetFit/sst2':\n", "            print(\"val labeled data len: \", len(val_dataset))\n", "    if args.backbone == 'roberta':\n", "        tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n", "    elif args.backbone == 'gpt2':\n", "        tokenizer = AutoTokenizer.from_pretrained('gpt2')\n", "        tokenizer.pad_token = tokenizer.eos_token\n", "    elif args.backbone == 'bert':\n", "        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n", "    else:\n", "        raise Exception(\"backbone should be roberta or gpt2\")\n", "    map_batch_size = min(1024, len(train_dataset), len(val_dataset), len(test_dataset))\n", "    train_dataset = preprocess(train_dataset, args.dataset, CFG.dataset_config[args.dataset][\"text_column\"], CFG.dataset_config[args.dataset][\"label_column\"])\n", "    val_dataset = preprocess(val_dataset, args.dataset, CFG.dataset_config[args.dataset][\"text_column\"], CFG.dataset_config[args.dataset][\"label_column\"])\n", "    encoded_train_dataset = train_dataset.map(\n", "        lambda e: tokenizer(e[CFG.dataset_config[args.dataset][\"text_column\"]], padding=True, truncation=True, max_length=args.max_length), batched=True,\n", "        batch_size=map_batch_size)\n", "    encoded_val_dataset = val_dataset.map(\n", "        lambda e: tokenizer(e[CFG.dataset_config[args.dataset][\"text_column\"]], padding=True, truncation=True, max_length=args.max_length), batched=True,\n", "        batch_size=map_batch_size)\n", "    \n", "    encoded_train_dataset = encoded_train_dataset.remove_columns([CFG.dataset_config[args.dataset][\"text_column\"]])\n", "    encoded_val_dataset = encoded_val_dataset.remove_columns([CFG.dataset_config[args.dataset][\"text_column\"]])\n\n", "    # HuggingFace map operations are lazy so we access the entire dataset to ensure the operations are applied\n", "    encoded_train_dataset = encoded_train_dataset[:len(encoded_train_dataset)]\n", "    encoded_val_dataset = encoded_val_dataset[:len(encoded_val_dataset)]\n", "    concept_set = CFG.concept_set[args.dataset]\n", "    print(\"concept len: \", len(concept_set))\n", "    d_name = args.dataset.replace('/', '_')\n", "    prefix = \"./\"\n", "    if args.labeling == 'mpnet':\n", "        prefix += \"mpnet_acs\"\n", "    elif args.labeling == 'simcse':\n", "        prefix += \"simcse_acs\"\n", "    elif args.labeling == 'angle':\n", "        prefix += \"angle_acs\"\n", "    elif args.labeling == 'llm':\n", "        prefix += \"llm_labeling\"\n", "    prefix += \"/\"\n", "    prefix += d_name\n", "    prefix += \"/\"\n", "    train_similarity = np.load(prefix + \"/concept_labels_train.npy\")\n", "    val_similarity = np.load(prefix + \"/concept_labels_val.npy\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    if args.automatic_concept_correction:\n", "        start = time.time()\n", "        print(\"training intervention...\")\n", "        # Convert label list to NumPy array\n", "        train_labels = np.array(encoded_train_dataset[CFG.dataset_config[args.dataset][\"label_column\"]])\n", "        \n", "        # Get all concept labels\n", "        concept_labels = np.array([get_labels(j, args.dataset) for j in range(len(concept_set))])  # shape: (num_concepts,)\n", "        \n", "        # Create a (num_samples, num_concepts) label match matrix\n", "        label_matches = train_labels[:, None] == concept_labels[None, :]  # shape: (num_samples, num_concepts)\n", "        \n", "        # Zero out all mismatched labels\n", "        train_similarity[~label_matches] = 0.0\n", "        \n", "        # Clip negative similarities to 0\n", "        np.maximum(train_similarity, 0.0, out=train_similarity)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        val_labels = np.array(encoded_val_dataset[CFG.dataset_config[args.dataset][\"label_column\"]])\n", "        \n", "        # Get all concept labels\n", "        concept_labels = np.array([get_labels(j, args.dataset) for j in range(len(concept_set))])  # shape: (num_concepts,)\n", "        \n", "        # Create a (num_samples, num_concepts) label match matrix\n", "        label_matches = val_labels[:, None] == concept_labels[None, :]  # shape: (num_samples, num_concepts)\n", "        \n", "        # Zero out all mismatched labels\n", "        val_similarity[~label_matches] = 0.0\n", "        \n", "        # Clip negative similarities to 0\n", "        np.maximum(val_similarity, 0.0, out=val_similarity)\n", "        \n", "        end = time.time()\n", "        print(\"time of training intervention:\", (end - start) / 3600, \"hours\")\n", "    print(\"creating loader...\")\n", "    train_loader = build_loaders(encoded_train_dataset, train_similarity, mode=\"train\")\n", "    val_loader = build_loaders(encoded_val_dataset, val_similarity, mode=\"valid\")\n", "    if args.backbone == 'roberta':\n", "        if args.tune_cbl_only:\n", "            print(\"preparing CBL only...\")\n", "            cbl = CBL(len(concept_set), args.dropout).to(device)\n", "            preLM = AutoModel.from_pretrained('roberta-base').to(device)\n", "            preLM.eval()\n", "            optimizer = torch.optim.Adam(cbl.parameters(), lr=1e-4)\n", "        else:\n", "            print(\"preparing backbone(roberta)+CBL...\")\n", "            backbone_cbl = RobertaCBL(len(concept_set), args.dropout).to(device)\n", "            optimizer = torch.optim.Adam(backbone_cbl.parameters(), lr=5e-6)\n", "    elif args.backbone == 'gpt2':\n", "        if args.tune_cbl_only:\n", "            print(\"preparing CBL only...\")\n", "            cbl = CBL(len(concept_set), args.dropout).to(device)\n", "            preLM = AutoModel.from_pretrained('gpt2').to(device)\n", "            preLM.eval()\n", "            optimizer = torch.optim.Adam(cbl.parameters(), lr=1e-4)\n", "        else:\n", "            print(\"preparing backbone(gpt2)+CBL...\")\n", "            backbone_cbl = GPT2CBL(len(concept_set), args.dropout).to(device)\n", "            optimizer = torch.optim.Adam(backbone_cbl.parameters(), lr=5e-6)\n", "    elif args.backbone == 'bert':\n", "        if args.tune_cbl_only:\n", "            print(\"preparing CBL only...\")\n", "            cbl = CBL(len(concept_set), args.dropout).to(device)\n", "            preLM = AutoModel.from_pretrained('bert-base-uncased').to(device)\n", "            preLM.eval()\n", "            optimizer = torch.optim.Adam(cbl.parameters(), lr=1e-4)\n", "        else:\n", "            print(\"preparing backbone(bert)+CBL...\")\n", "            backbone_cbl = BERTCBL(len(concept_set), args.dropout).to(device)\n", "            optimizer = torch.optim.Adam(backbone_cbl.parameters(), lr=5e-6)\n", "    else:\n", "        raise Exception(\"backbone should be roberta or gpt2\")\n", "    print(\"start training...\")\n", "    best_loss = float('inf')\n", "    if args.backbone == 'roberta':\n", "        prefix += 'roberta_cbm'\n", "    elif args.backbone == 'gpt2':\n", "        prefix += 'gpt2_cbm'\n", "    elif args.backbone == 'bert':\n", "        prefix += 'bert_cbm'\n", "    prefix += \"/\"\n", "    if not os.path.exists(prefix):\n", "        os.makedirs(prefix)\n", "    model_name = \"cbl\"\n", "    if args.tune_cbl_only:\n", "        model_name += \"_no_backbone\"\n", "    if args.automatic_concept_correction:\n", "        model_name += \"_acc\"\n", "    start = time.time()\n", "    if args.labeling == 'llm':\n", "        epochs = 10\n", "    else:\n", "        epochs = CFG.cbl_epochs[args.dataset]\n", "    for e in range(epochs):\n", "        print(\"Epoch \", e+1, \":\")\n", "        if args.tune_cbl_only:\n", "            cbl.train()\n", "        else:\n", "            backbone_cbl.train()\n", "        training_loss = []\n", "        for i, batch in enumerate(train_loader):\n", "            batch_text, batch_sim = batch[0], batch[1]\n", "            batch_text = {k: v.to(device) for k, v in batch_text.items()}\n", "            batch_sim = batch_sim.to(device)\n", "            if args.tune_cbl_only:\n", "                with torch.no_grad():\n", "                    LM_features = preLM(input_ids=batch_text[\"input_ids\"], attention_mask=batch_text[\"attention_mask\"]).last_hidden_state\n", "                    if args.backbone == 'roberta' or args.backbone == 'bert':\n", "                        LM_features = LM_features[:, 0, :]\n", "                    elif args.backbone == 'gpt2':\n", "                        LM_features = eos_pooling(LM_features, batch_text[\"attention_mask\"])\n", "                    else:\n", "                        raise Exception(\"backbone should be roberta or gpt2\")\n", "                cbl_features = cbl(LM_features)\n", "            else:\n", "                cbl_features = backbone_cbl(batch_text)\n", "            optimizer.zero_grad()\n", "            loss = -cos_sim_cubed(cbl_features, batch_sim)\n", "            loss.backward()\n", "            optimizer.step()\n", "            print(\"batch \", str(i), \" loss: \", loss.detach().cpu().numpy(), end=\"\\r\")\n", "            training_loss.append(loss.detach().cpu().numpy())\n", "        avg_training_loss = sum(training_loss)/len(training_loss)\n", "        print(\"training loss: \", avg_training_loss)\n", "        if args.tune_cbl_only:\n", "            cbl.eval()\n", "        else:\n", "            backbone_cbl.eval()\n", "        val_loss = []\n", "        for batch in val_loader:\n", "            batch_text, batch_sim = batch[0], batch[1]\n", "            batch_text = {k: v.to(device) for k, v in batch_text.items()}\n", "            batch_sim = batch_sim.to(device)\n", "            with torch.no_grad():\n", "                if args.tune_cbl_only:\n", "                    LM_features = preLM(input_ids=batch_text[\"input_ids\"], attention_mask=batch_text[\"attention_mask\"]).last_hidden_state\n", "                    if args.backbone == 'roberta' or args.backbone == 'bert':\n", "                        LM_features = LM_features[:, 0, :]\n", "                    elif args.backbone == 'gpt2':\n", "                        LM_features = eos_pooling(LM_features, batch_text[\"attention_mask\"])\n", "                    else:\n", "                        raise Exception(\"backbone should be roberta or gpt2\")\n", "                    cbl_features = cbl(LM_features)\n", "                else:\n", "                    cbl_features = backbone_cbl(batch_text)\n", "                loss = -cos_sim_cubed(cbl_features, batch_sim)\n", "                val_loss.append(loss.detach().cpu().numpy())\n", "        avg_val_loss = sum(val_loss)/len(val_loss)\n", "        print(\"val loss: \", avg_val_loss)\n", "        if avg_val_loss < best_loss:\n", "            print(\"save model\")\n", "            best_loss = avg_val_loss\n", "            if args.tune_cbl_only:\n", "                torch.save(cbl.state_dict(), prefix + model_name + \".pt\")\n", "            else:\n", "                torch.save(backbone_cbl.state_dict(), prefix + model_name + \".pt\")\n", "    end = time.time()\n", "    print(\"time of training CBL:\", (end - start) / 3600, \"hours\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}